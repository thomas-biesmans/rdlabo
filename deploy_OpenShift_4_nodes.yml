--- 
- import_playbook: tasks/check_all.yml


- name: Check OpenShift deployment status
  hosts: openshift_clusters
  gather_facts: false
  vars_files:
  - passwords/ansible_vault.yml
  - vars/network-config.yml
  - group_vars/vcenter_vms.yml
  environment:
    KUBECONFIG: "{{ vars.openshift_config_dir }}/auth/kubeconfig"
  tasks:

  - name: Check whether OpenShift cluster exists already
    tags: deploy_cluster
    delegate_to: localhost
    block:

    - name: Check whether OpenShift cluster exists already
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Namespace
        name: openshift-kube-apiserver
      register: namespace_kube_apiserver
      timeout: 10
      retries: 3
      ignore_errors: yes

    - name: "Log when namespace 'openshift-kube-apiserver' already exists"
      debug:
        msg: "Namespace 'openshift-kube-apiserver' already exists, no need to redeploy."
      when: not namespace_kube_apiserver.failed and namespace_kube_apiserver.resources[0].status.phase | default('') == "Active"


- name: Deploy VMs
  hosts: openshift_clusters
  gather_facts: false
  vars_files:
  - passwords/ansible_vault.yml
  - vars/network-config.yml
  tasks:

  - name: Combine customized VM config
    set_fact:
      community_vmware_vmware__guest: "{{ default_community_vmware_vmware__guest | combine(custom_community_vmware_vmware__guest | default({}), recursive=true) }}"
      ansible_becomes_password: "{{ client_password }}"
    tags: always

  - name: Gather hostvars as dict
    set_fact:
      hostvars_dict: "{{ hostvars }}"
    tags: always

  # - name: Ensure Ansible become password is set
  #   set_fact:
  #     ansible_become_password: "{{ deployment_password }}"
  #   when: ansible_become_password is undefined or ansible_become_password != deployment_password
  #   tags: always

  # - name: biep
  #   debug:
  #     msg: "{{ vars.community_vmware_vmware__guest }}"

  - name: Deploy base infrastructure
    # when: vars.namespace_kube_apiserver.failed or vars.namespace_kube_apiserver.resources[0].status.phase != "Active"
    tags: deploy_infrastructure
    block:

    - name: Deploy DNS records for nodes
      ansible.builtin.import_tasks: tasks/deploy-windows-dns-records.yml

    - name: Deploy DNS records for OpenShift cluster
      ansible.builtin.import_tasks: tasks/deploy-windows-dns-records-openshift-cluster.yml
      
    - name: Deploy DHCP reservations
      ansible.builtin.import_tasks: tasks/deploy-windows-dhcp-reservations.yml

    - name: Deploy NSX segments
      ansible.builtin.include_tasks:
        file: tasks/deploy-vmware-nsx-segments.yml
        apply:
          tags: vm
      tags: vm
      when: nsx_integration
      
    - name: Deploy VMs
      ansible.builtin.include_tasks:
        file: tasks/deploy-vmware-vms-from-template.yml
        apply:
          tags: vm
      tags: vm


- name: Prepare the web server
  hosts: openshift_bootstrap_nodes
  gather_facts: false
  become: true
  vars_files:
  - passwords/ansible_vault.yml
  - vars/network-config.yml
  - group_vars/vcenter_vms.yml
  serial: 1
  tasks:

  - name: Prepare bootstrap environment
    when: vars.namespace_kube_apiserver.failed or vars.namespace_kube_apiserver.resources[0].status.phase != "Active"
    tags: prep_bootstrap
    delegate_to: localhost
    block:

    - name: Install podman
      ansible.builtin.yum:
        name: podman
        state: latest
      tags: web
      run_once: true

    - name: Collect firewalld status
      service_facts:
      register: services_state

    - name: Allow traffic to httpd on bastion port
      ansible.posix.firewalld:
        zone: public
        port: "{{ vars.bastion_port }}/tcp"
        permanent: yes
        state: enabled
      when: services_state.ansible_facts.services['firewalld.service'].status == 'enabled'
      tags: web
      vars:
        ansible_python_interpreter: /usr/bin/python3.6


- name: Prepare OpenShift deployment
  hosts: openshift_bootstrap_nodes
  gather_facts: false
  vars_files:
  - passwords/ansible_vault.yml
  - vars/network-config.yml
  - group_vars/vcenter_vms.yml
  tasks:


  - name: Prepare bootstrap environment
    when: vars.namespace_kube_apiserver.failed or vars.namespace_kube_apiserver.resources[0].status.phase != "Active"
    tags: prep_bootstrap
    delegate_to: localhost
    block:

    - name: OpenShift deployment preparation
      tags: openshift
      block:

      - name: Create config & install dir
        file:
          path: "{{ item }}"
          state: directory
        with_items:
        - "{{ vars.openshift_config_dir }}"
        - "{{ vars.openshift_download_dir }}"
        - "{{ vars.openshift_download_dir }}/nitro-python"
        - "{{ vars.openshift_download_dir }}/nitro-python/extract"


      - name: Download specific sources
        get_url:
          url: "{{ vars.openshift_custom_version_source }}"
          dest: "{{ vars.openshift_download_dir }}/openshift-install-linux.tar.gz"
        when: vars.openshift_custom_version_source | length > 0

      - name: Download default stable sources if no specific version is set
        get_url:
          url: "{{ vars.openshift_default_source }}"
          dest: "{{ vars.openshift_download_dir }}/openshift-install-linux.tar.gz"
        when: vars.openshift_custom_version_source | length <= 0
      
      - name: Untar the installation file
        unarchive:
          src: "{{ vars.openshift_download_dir }}/openshift-install-linux.tar.gz"
          dest: "{{ vars.openshift_download_dir }}"
          creates: "{{ vars.openshift_download_dir }}/openshift-install"


      - name: Download default stable client
        get_url:
          url: "{{ vars.openshift_default_client_source }}"
          dest: "{{ vars.openshift_download_dir }}/openshift-client-linux.tar.gz"
      
      - name: Untar the client file
        unarchive:
          src: "{{ vars.openshift_download_dir }}/openshift-client-linux.tar.gz"
          dest: "{{ vars.openshift_download_dir }}"
          creates: "{{ vars.openshift_download_dir }}/oc"
      
      # - name: Copy client files to the local user's bin, instead of /usr/local/bin which would need become
      #   copy:
      #     src: "{{ item }}"
      #     dest: $HOME/bin/
      #   with_items: 
      #   - "{{ vars.openshift_download_dir }}/oc"
      #   - "{{ vars.openshift_download_dir }}/kubectl"

      - name: Copy client files to /usr/local/bin for all users
        copy:
          src: "{{ item }}"
          dest: /usr/local/bin
        with_items: 
        - "{{ vars.openshift_download_dir }}/oc"
        - "{{ vars.openshift_download_dir }}/kubectl"

      - name: Check whether new SSH key is already created
        include_tasks: tasks/check_local_ssh_key_creation.yml
        run_once: true

      - name: Read SSH key
        slurp:
          src: "~/.ssh/{{ vars.deployment_ssh_filename }}.pub"
        register: sshKey
        
      - name: Check whether manifests were already created
        stat:
          path: "{{ vars.openshift_config_dir }}/manifests"
        register: manifests_dir

      - name: Check whether ignition files were already created
        stat:
          path: "{{ vars.openshift_config_dir }}/master.ign"
        register: ignition_file

      - name: Create manifests
        when: not manifests_dir.stat.exists and not ignition_file.stat.exists
        block:

        - name: Template out the inventory file to the config dir
          template:
            src: templates/openshift_4_upi_install_config_vmware_template.yml
            dest: "{{ vars.openshift_config_dir }}/install-config.yaml"
            force: no

        - name: Take a copy of the install-config as a backup
          copy:
            src: "{{ vars.openshift_config_dir }}/install-config.yaml"
            dest: "{{ vars.openshift_config_dir }}/install-config.yaml.bak"
            force: no

        - name: Create manifest files
          shell: "{{ vars.openshift_download_dir }}/openshift-install create manifests --dir {{ vars.openshift_config_dir }}"
    
        - name: Remove unneeded manifest files
          file:
            path: "{{ item }}"
            state: absent
          with_fileglob:
          - "{{ vars.openshift_config_dir }}/openshift/99_openshift-cluster-api_master-machines-*.yaml"
          - "{{ vars.openshift_config_dir }}/openshift/99_openshift-cluster-api_worker-machineset-*.yaml"
          
        - name: Set masters to non-Schedulable
          replace:
            path: "{{ vars.openshift_config_dir }}/manifests/cluster-scheduler-02-config.yml"
            regexp: 'mastersSchedulable: true'
            replace: 'mastersSchedulable: false'


      - name: Create ignition files
        shell: "{{ vars.openshift_download_dir }}/openshift-install create ignition-configs --dir {{ vars.openshift_config_dir }}"
        when: not ignition_file.stat.exists
  
      - name: Query the cluster's random identifier
        shell: "jq -r .infraID {{ vars.openshift_config_dir }}/metadata.json"
        register: cluster_id

      - name: Report cluster name
        debug:
          msg: "Cluster ID: {{ cluster_id.stdout }}"


    - name: Configure a web server for the ignition files
      tags:
      - ignition_prep
      block:
    
        - name: Pull Apache httpd container image
          containers.podman.podman_image:
            name: httpd
            pull: true
            tag: latest
    
        - name: Configure webroot - permissions
          file:
            path: "{{ vars.bastion_webroot }}"
            state: directory
            mode: '0777'

        - name: Configure webroot - SELinux file context, should be idempotent, isn't...
          become: yes
          community.general.sefcontext:
            target: "{{ vars.bastion_webroot }}(/.*)?"
            setype: "container_share_t"
            state: present
          register: webroot_dir_fcontext
          vars:
            ansible_python_interpreter: /usr/bin/python3.6

        - name: Create a custom index.html
          copy:
            dest: "{{ vars.bastion_webroot }}/index.html"
            content: |
              Custom Web Page, hosting ignition files
    
        - name: Copy the ignition file
          copy:
            src: "{{ vars.openshift_config_dir }}/{{ item }}.ign"
            dest: "{{ vars.bastion_webroot }}/{{ item }}.ign"
          with_items:
          - "master"
          - "worker"
          - "bootstrap"
    
        - name: Apply SELinux fcontext if it wasn't already added
          become: yes
          shell: restorecon -irv {{ vars.bastion_webroot }}
          when: webroot_dir_fcontext.changed
    
        - name: Run Apache httpd container
          containers.podman.podman_container:
            name: "{{ vars.bastion_container_name }}"
            image: httpd
            state: started
            detach: true
            expose:
            - 80
            ports:
            - "{{ vars.bastion_port }}:80"
            volume:
            - "{{ vars.bastion_webroot }}:/usr/local/apache2/htdocs/:exec"

        - name: Query index.html
          shell: "curl http://{{ vars.bastion_ip }}:{{ vars.bastion_port }}/index.html"
          register: index_curl

        - name: Show output received from web server
          debug:
            msg: "Output index.html: {{ index_curl.stdout }}"

        - name: Query bootstrap ignition
          shell: "curl http://{{ vars.bastion_ip }}:{{ vars.bastion_port }}/bootstrap.ign"
          register: bootstrap_curl

        - name: Show output received from web server
          debug:
            msg: "Output bootstrap.ign: {{ bootstrap_curl.stdout[:12] }}"

        - name: Stop if ignition config isn't available
          fail:
          when: bootstrap_curl.stdout[:12] != "{\"ignition\":"

        - name: Template out the inventory file to the config dir
          template:
            src: templates/openshift_4_upi_ignition_template.yml
            dest: "{{ vars.openshift_config_dir }}/{{ item }}.ign"
            force: no
          with_items:
          - "merge-bootstrap"

        - name: Encode them in Base64
          # shell: "base64 -w0 > {{ vars.openshift_config_dir }}/{{ item }}.64"
          copy:
            content: "{{ lookup('file', '{{ vars.openshift_config_dir }}/{{ item }}.ign') | b64encode }}"
            dest: "{{ vars.openshift_config_dir }}/{{ item }}.64"
          with_items:
          - "master"
          - "worker"
          - "merge-bootstrap"


- name: Deploy ignition configs on masters
  hosts: openshift_master_nodes
  gather_facts: false
  vars_files:
  - passwords/ansible_vault.yml
  - vars/network-config.yml
  tasks:


  - name: Prepare nodes
    # when: vars.namespace_kube_apiserver.failed or vars.namespace_kube_apiserver.resources[0].status.phase != "Active"
    tags: prep_nodes
    block:

    - name: Deploy ignition configs
      tags: ignition_vm_deploy
      block:

      - name: Update VMs' Advanced Settings with Base64 encoded ignition data
        throttle: "{{ parallel_deployments }}"
        retries: 3
        delay: 10
        community.vmware.vmware_guest:
          validate_certs: "{{ validate_certs | default(false) }}"
          hostname: "{{ vcenter_hostname | default(lookup('env', 'VMWARE_HOST')) }}"
          username: "{{ vcenter_username | default(lookup('env', 'VMWARE_USER')) }}"
          password: "{{ vcenter_password | default(lookup('env', 'VMWARE_PASSWORD')) }}"
          name: "{{ inventory_hostname_short }}"
          state: present
          advanced_settings:
          - key: guestinfo.ignition.config.data
            value: "{{ lookup('file', '{{ vars.openshift_config_dir }}/{{ item }}.ign') | b64encode }}"

        delegate_to: localhost
        with_items:
        - "master"

- name: Deploy ignition configs on workers
  hosts: openshift_worker_nodes, openshift_infra_nodes
  gather_facts: false
  vars_files:
  - passwords/ansible_vault.yml
  - vars/network-config.yml
  tasks:

  - name: Prepare nodes
    # when: vars.namespace_kube_apiserver.failed or vars.namespace_kube_apiserver.resources[0].status.phase != "Active"
    tags: prep_nodes
    block:

    - name: Update VMs' Advanced Settings with Base64 encoded ignition data
      throttle: "{{ parallel_deployments }}"
      retries: 3
      delay: 10
      community.vmware.vmware_guest:
        validate_certs: "{{ validate_certs | default(false) }}"
        hostname: "{{ vcenter_hostname | default(lookup('env', 'VMWARE_HOST')) }}"
        username: "{{ vcenter_username | default(lookup('env', 'VMWARE_USER')) }}"
        password: "{{ vcenter_password | default(lookup('env', 'VMWARE_PASSWORD')) }}"
        name: "{{ inventory_hostname_short }}"
        state: present
        advanced_settings:
        - key: guestinfo.ignition.config.data
          value: "{{ lookup('file', '{{ vars.openshift_config_dir }}/{{ item }}.ign') | b64encode }}"

      delegate_to: localhost
      tags: ignition_vm_deploy
      with_items:
      - "worker"


- name: Deploy ignition configs on bootstrap node
  hosts: openshift_bootstrap_nodes
  gather_facts: false
  vars_files:
  - passwords/ansible_vault.yml
  - vars/network-config.yml
  tasks:

  - name: Prepare nodes
    # when: vars.namespace_kube_apiserver.failed or vars.namespace_kube_apiserver.resources[0].status.phase != "Active"
    tags: prep_nodes
    block:

    - name: Update VMs' Advanced Settings with Base64 encoded ignition data
      throttle: "{{ parallel_deployments }}"
      retries: 3
      delay: 10
      community.vmware.vmware_guest:
        validate_certs: "{{ validate_certs | default(false) }}"
        hostname: "{{ vcenter_hostname | default(lookup('env', 'VMWARE_HOST')) }}"
        username: "{{ vcenter_username | default(lookup('env', 'VMWARE_USER')) }}"
        password: "{{ vcenter_password | default(lookup('env', 'VMWARE_PASSWORD')) }}"
        name: "{{ inventory_hostname_short }}"
        state: present
        advanced_settings:
        - key: guestinfo.ignition.config.data
          value: "{{ lookup('file', '{{ vars.openshift_config_dir }}/{{ item }}.ign') | b64encode }}"

      delegate_to: localhost
      tags: ignition_vm_deploy
      with_items:
      - "merge-bootstrap"


- name: Deploy the Citrix load balancers
  hosts: openshift_bootstrap_nodes
  gather_facts: false
  vars_files:
  - passwords/ansible_vault.yml
  - vars/network-config.yml
  - group_vars/citrix_load_balancer.yml
  tags: prep_citrix
  tasks:

  # - name: Log into Citrix to be able to download the SDK
  #   ansible.builtin.uri:
  #     url: "http://{{ citrix_ip }}"
  #     method: POST
  #     body_format: form-urlencoded
  #     body:
  #       action: "/login/do_login"
  #       username: "{{ citrix_username_unecnrypted }}"
  #       password: "{{ citrix_password_unecnrypted }}"
  #       enter: Log On
  #     status_code: 200
  #   register: login

  # - name: Download the SDK using a previously stored cookie
  #   ansible.builtin.uri:
  #     url: "http://{{ citrix_ip }}/nitro-python.tgz"
  #     user: "{{ citrix_username_unecnrypted }}"
  #     password: "{{ citrix_password_unecnrypted }}"
  #     dest: "{{ vars.openshift_download_dir }}/nitro-python.tgz"
  #     force_basic_auth: yes
  #     # creates: "{{ vars.openshift_download_dir }}/nitro-python.tgz"
  #     method: GET
  #     return_content: yes
  #     headers:
  #       Cookie: "{{ login.cookies_string }}"

  - name: Prep Citrix SDK
    delegate_to: localhost
    run_once: true
    block:
      
    - name: Check whether Citrix SDK was downloaded and extracted from http://<ip>/nitro-python.tgz
      stat:
        path: "{{ vars.openshift_download_dir }}/nitro-python/extract/setup.py"
      register: citrix_sdk_file

    - name: Download and extract the SDK
      when: not citrix_sdk_file.stat.exists 
      block:
        
      - name: Check whether Citrix SDK was downloaded from http://<ip>/nitro-python.tgz
        stat:
          path: "{{ vars.openshift_download_dir }}/nitro-python.tgz"
        register: citrix_sdk_downloaded_archive

      - name: Stop if file wasn't downloaded yet
        fail:
          msg: "Download the Citrix SDK for Python from the ADC > Downloads > SDK for Python"
        when: not citrix_sdk_downloaded_archive.stat.exists 

      # - name: Download specific sources
      #   get_url:
      #     url: "http://{{ citrix_ip }}/nitro-python.tgz"
      #     dest: "{{ vars.openshift_download_dir }}/nitro-python.tgz"
      #     validate_certs: no
        # when: vars.openshift_custom_version_source | length > 0

      - name: Untar the Citrix SDK
        unarchive:
          src: "{{ vars.openshift_download_dir }}/nitro-python.tgz"
          dest: "{{ vars.openshift_download_dir }}/nitro-python"
          creates: "{{ vars.openshift_download_dir }}/nitro-python/extract/setup.py"

      - name: Find archives inside of extracted dir
        find:
          paths: "{{ vars.openshift_download_dir }}/nitro-python"
          patterns: "ns_nitro-python*.tar"
        register: citrix_sdk_nested_tar

      - name: Untar the Citrix SDK
        unarchive:
          src: "{{ citrix_sdk_nested_tar.files[0].path }}"
          dest: "{{ vars.openshift_download_dir }}/nitro-python/"
          creates: "{{ vars.openshift_download_dir }}/nitro-python/extract/setup.py"

      - name: Find extracted directory inside of extracted dir
        find:
          paths: "{{ vars.openshift_download_dir }}/nitro-python"
          file_type: directory
          patterns: "nitro-python-*"
        register: citrix_sdk_nested_tar_dir

      - name: Copy extracted nested dir
        ansible.posix.synchronize:
          src: "{{ citrix_sdk_nested_tar_dir.files[0].path }}/"
          dest: "{{ vars.openshift_download_dir }}/nitro-python/extract/"

      - name: Remove nested dir
        file:
          path: "{{ citrix_sdk_nested_tar_dir.files[0].path }}"
          state: absent

    - name: Ensure Citrix SDK is installed
      become: true
      run_once: true
      ansible.builtin.shell:
        cmd: "python3.8 setup.py install"
        chdir: "{{ vars.openshift_download_dir }}/nitro-python/extract/"


  - name: Create Citrix Load-Balancers
    delegate_to: localhost
    block:

    - name: Reset API service group
      set_fact:
        temp_servicegroup_members_api: []

    - name: Build load balancer topology - API service group
      set_fact:
        temp_servicegroup_members_api: '{{ temp_servicegroup_members_api + [{
          "ip": hostvars[item].ip,
          "port": openshift_api_port,
          "weight": 1
        }] }}'
      when: hostvars[item].openshift_cluster_name == vars.openshift_cluster_name
      with_items:
      - "{{ groups.openshift_master_nodes }}"
      - "{{ groups.openshift_bootstrap_nodes }}"

    - name: Reset API ignition service group
      set_fact:
        temp_servicegroup_members_api_ignition: []

    - name: Build load balancer topology - API ignition service group
      set_fact:
        temp_servicegroup_members_api_ignition: '{{ temp_servicegroup_members_api_ignition + [{
          "ip": hostvars[item].ip,
          "port": openshift_api_ignition_port,
          "weight": 1
        }] }}'
      when: hostvars[item].openshift_cluster_name == vars.openshift_cluster_name
      with_items:
      - "{{ groups.openshift_master_nodes }}"
      - "{{ groups.openshift_bootstrap_nodes }}"


    - name: Reset Apps service group
      set_fact:
        temp_servicegroup_members_apps: []

    - name: Build load balancer topology - Apps service group
      set_fact:
        temp_servicegroup_members_apps: '{{ temp_servicegroup_members_apps + [{
          "ip": hostvars[item].ip,
          "port": 65535,
          "weight": 1
        }] }}'
      when: hostvars[item].openshift_cluster_name == vars.openshift_cluster_name
      with_items:
      - "{{ groups.openshift_worker_nodes }}"

    - name: Build load balancer topology - API load balancer
      set_fact:
        citrix_vservers_api:
        - name: "lb_openshift_{{ vars.openshift_cluster_name }}_api"
          servicetype: SSL_BRIDGE
          ipaddress: "{{ openshift_api_ip }}"
          port: "{{ openshift_api_port | int }}"
          servicegroup:
            name: "lb_svg_openshift_{{ vars.openshift_cluster_name }}_api"
            servicetype: SSL_BRIDGE
            mode: exact
            members: "{{ temp_servicegroup_members_api }}"

        citrix_vservers_api_ignition:
        - name: "lb_openshift_{{ vars.openshift_cluster_name }}_api_ignition"
          servicetype: SSL_BRIDGE
          ipaddress: "{{ openshift_api_ip }}"
          port: "{{ openshift_api_ignition_port | int }}"
          servicegroup:
            name: "lb_svg_openshift_{{ vars.openshift_cluster_name }}_api_ignition"
            servicetype: SSL_BRIDGE
            mode: exact
            members: "{{ temp_servicegroup_members_api_ignition }}"

        citrix_vservers_apps:
        - name: "lb_openshift_{{ vars.openshift_cluster_name }}_apps_any"
          servicetype: ANY
          ipaddress: "{{ vars.openshift_ingress_ip }}"
          port: 65535
          servicegroup:
            name: "lb_svg_openshift_{{ vars.openshift_cluster_name }}_apps"
            servicetype: ANY
            mode: exact
            members: "{{ temp_servicegroup_members_apps }}"

    - name: Build load balancer topology - Create list
      set_fact:
        citrix_vservers: "{{ citrix_vservers_api + citrix_vservers_api_ignition + citrix_vservers_apps }}"

    - name: Display load balancer configuration
      debug:
        msg: "{{ citrix_vservers }}"

    - name: Deploy Service Groups
      ansible.builtin.import_tasks: tasks/deploy-citrix-servicegroups.yml

    - name: Deploy Load Balancers
      ansible.builtin.import_tasks: tasks/deploy-citrix-vservers.yml


- name: Power on bootstrap node if bootstrap is needed
  hosts: openshift_bootstrap_nodes
  gather_facts: false
  vars_files:
  - passwords/ansible_vault.yml
  - vars/network-config.yml
  tasks:

  - name: Start the bootstrap node
    when: vars.namespace_kube_apiserver.failed or vars.namespace_kube_apiserver.resources[0].status.phase != "Active"
    tags: deploy_cluster
    block:

    - name: Power on bootstrap node if bootstrap is needed
      retries: 3
      delay: 10
      community.vmware.vmware_guest:
        validate_certs: "{{ validate_certs | default(false) }}"
        hostname: "{{ vcenter_hostname | default(lookup('env', 'VMWARE_HOST')) }}"
        username: "{{ vcenter_username | default(lookup('env', 'VMWARE_USER')) }}"
        password: "{{ vcenter_password | default(lookup('env', 'VMWARE_PASSWORD')) }}"
        name: "{{ inventory_hostname_short }}"
        state: poweredon
      delegate_to: localhost

    - name: Query ignition files served by the bootstrap node, waiting up to 5-7,5 minutes
      shell: "curl -k https://{{ openshift_api_url }}:22623/config/master"
      retries: 5
      delay: 30 # default curl timeout is 60s, but is not always triggered
      until: ignition_curl.stdout[:20] == "{\"ignition\":{\"versio"
      register: ignition_curl
      delegate_to: localhost

    - name: Show output received from bootstrapped cluster
      debug:
        msg: "Output: {{ ignition_curl.stdout[:20] }}"


- name: Power on master nodes
  hosts: openshift_master_nodes
  gather_facts: false
  vars_files:
  - passwords/ansible_vault.yml
  - vars/network-config.yml
  tasks:

  - name: Start the master nodes if bootstrap is needed
    # when: vars.namespace_kube_apiserver.failed or vars.namespace_kube_apiserver.resources[0].status.phase != "Active"
    tags: deploy_cluster
    block:

    - name: Power on master nodes if bootstrap is needed
      retries: 3
      delay: 10
      community.vmware.vmware_guest:
        validate_certs: "{{ validate_certs | default(false) }}"
        hostname: "{{ vcenter_hostname | default(lookup('env', 'VMWARE_HOST')) }}"
        username: "{{ vcenter_username | default(lookup('env', 'VMWARE_USER')) }}"
        password: "{{ vcenter_password | default(lookup('env', 'VMWARE_PASSWORD')) }}"
        name: "{{ inventory_hostname_short }}"
        state: poweredon
      delegate_to: localhost


- name: Power on worker nodes
  hosts: openshift_worker_nodes
  gather_facts: false
  vars_files:
  - passwords/ansible_vault.yml
  - vars/network-config.yml
  tasks:

  - name: Start the worker nodes if bootstrap is needed
    # when: vars.namespace_kube_apiserver.failed or vars.namespace_kube_apiserver.resources[0].status.phase != "Active"
    tags: deploy_cluster
    block:

    - name: Power on worker nodes if bootstrap is needed
      retries: 3
      delay: 10
      community.vmware.vmware_guest:
        validate_certs: "{{ validate_certs | default(false) }}"
        hostname: "{{ vcenter_hostname | default(lookup('env', 'VMWARE_HOST')) }}"
        username: "{{ vcenter_username | default(lookup('env', 'VMWARE_USER')) }}"
        password: "{{ vcenter_password | default(lookup('env', 'VMWARE_PASSWORD')) }}"
        name: "{{ inventory_hostname_short }}"
        state: poweredon
      delegate_to: localhost


- name: Deploy OpenShift
  hosts: openshift_bootstrap_nodes
  gather_facts: false
  vars_files:
  - passwords/ansible_vault.yml
  - vars/network-config.yml
  - group_vars/vcenter_vms.yml
  environment:
    KUBECONFIG: "{{ vars.openshift_config_dir }}/auth/kubeconfig"
  tasks:

  - name: Deploy cluster 
    tags: deploy_cluster
    delegate_to: localhost
    block:

    - name: Start install wait-for bootstrap-complete (30 minute time-out each retry)
      shell: "{{ vars.openshift_download_dir }}/openshift-install --dir {{ vars.openshift_config_dir }} wait-for bootstrap-complete --log-level=debug"
      register: deploy_bootstrap_results
      retries: 2
      delay: 5
      until: '"level=info msg=It is now safe to remove the bootstrap resources" in deploy_bootstrap_results.stderr_lines'
      ignore_errors: yes

    - name: Fail / halt if there is an error with command parameters
      fail:
        msg: "Error with command parameters, help is shown"
      when: '"Wait for install-time events." in deploy_bootstrap_results.stdout_lines'

    - name: Fail / halt if bootstrap didn't complete
      fail:
        msg: ["Error with command, you can gather bootstrap information with the following command:",
              "{{ vars.openshift_download_dir }}/openshift-install --dir {{ vars.openshift_config_dir }} gather bootstrap
                --bootstrap {{ (hostvars_dict.values()
                  | selectattr('openshift_cluster_name', 'defined')
                  | selectattr('openshift_cluster_name', 'eq', vars.openshift_cluster_name)
                  | selectattr('type', 'eq', 'bootstrap')
                  | map(attribute='ip')
                  | list)[0] }}
                --master {{ (hostvars_dict.values()
                  | selectattr('openshift_cluster_name', 'defined')
                  | selectattr('openshift_cluster_name', 'eq', vars.openshift_cluster_name)
                  | selectattr('type', 'eq', 'master')
                  | map(attribute='ip')
                  | list)[0] }}
                --master {{ (hostvars_dict.values()
                  | selectattr('openshift_cluster_name', 'defined')
                  | selectattr('openshift_cluster_name', 'eq', vars.openshift_cluster_name)
                  | selectattr('type', 'eq', 'master')
                  | map(attribute='ip')
                  | list)[1] }}
                --master {{ (hostvars_dict.values()
                  | selectattr('openshift_cluster_name', 'defined')
                  | selectattr('openshift_cluster_name', 'eq', vars.openshift_cluster_name)
                  | selectattr('type', 'eq', 'master')
                  | map(attribute='ip')
                  | list)[2] }}
              "]
      when: 'not "level=info msg=It is now safe to remove the bootstrap resources" in deploy_bootstrap_results.stderr_lines'

    - name: Show wait-for bootstrap-complete output
      debug:
        msg: "{{ ['Command executed in ' + deploy_bootstrap_results.attempts | string + ' tries, output:',
                  '-----------------------------------'] +
                  deploy_bootstrap_results.stderr_lines }}"


- name: Power off bootstrap node if bootstrap is finished
  hosts: openshift_bootstrap_nodes
  gather_facts: false
  vars_files:
  - passwords/ansible_vault.yml
  - vars/network-config.yml
  tasks:

  - name: Power off the bootstrap node
    when: vars.deploy_bootstrap_results.stderr_lines is defined and '"level=info msg=It is now safe to remove the bootstrap resources" in vars.deploy_bootstrap_results.stderr_lines'
    tags: deploy_cluster
    block:

    - name: Power off bootstrap node if bootstrap is finished
      retries: 3
      delay: 10
      community.vmware.vmware_guest:
        validate_certs: "{{ validate_certs | default(false) }}"
        hostname: "{{ vcenter_hostname | default(lookup('env', 'VMWARE_HOST')) }}"
        username: "{{ vcenter_username | default(lookup('env', 'VMWARE_USER')) }}"
        password: "{{ vcenter_password | default(lookup('env', 'VMWARE_PASSWORD')) }}"
        name: "{{ inventory_hostname_short }}"
        state: poweredoff
      delegate_to: localhost


- name: Remove bootstrap from load balancers
  hosts: openshift_bootstrap_nodes
  gather_facts: false
  vars_files:
  - passwords/ansible_vault.yml
  - vars/network-config.yml
  - group_vars/citrix_load_balancer.yml
  tags: prep_citrix
  tasks:

  - name: Remove bootstrap from load balancers
    when: vars.deploy_bootstrap_results.stderr_lines is defined and '"level=info msg=It is now safe to remove the bootstrap resources" in vars.deploy_bootstrap_results.stderr_lines'
    tags: deploy_cluster
    delegate_to: localhost
    block:

    - name: Reset API service group
      set_fact:
        temp_servicegroup_members_api: []

    - name: Build load balancer topology - API service group
      set_fact:
        temp_servicegroup_members_api: '{{ temp_servicegroup_members_api + [{
          "ip": hostvars[item].ip,
          "port": openshift_api_port,
          "weight": 1
        }] }}'
      when: hostvars[item].openshift_cluster_name == vars.openshift_cluster_name
      with_items:
      - "{{ groups.openshift_master_nodes }}"

    - name: Reset API ignition service group
      set_fact:
        temp_servicegroup_members_api_ignition: []

    - name: Build load balancer topology - API ignition service group
      set_fact:
        temp_servicegroup_members_api_ignition: '{{ temp_servicegroup_members_api_ignition + [{
          "ip": hostvars[item].ip,
          "port": openshift_api_ignition_port,
          "weight": 1
        }] }}'
      when: hostvars[item].openshift_cluster_name == vars.openshift_cluster_name
      with_items:
      - "{{ groups.openshift_master_nodes }}"

    - name: Build load balancer topology - API load balancer
      set_fact:
        citrix_vservers_api:
        - name: "lb_openshift_{{ openshift_cluster_name }}_api"
          servicetype: SSL_BRIDGE
          ipaddress: "{{ openshift_api_ip }}"
          port: "{{ openshift_api_port | int }}"
          servicegroup:
            name: "lb_svg_openshift_{{ openshift_cluster_name }}_api"
            servicetype: SSL_BRIDGE
            mode: exact
            members: "{{ temp_servicegroup_members_api }}"

        citrix_vservers_api_ignition:
        - name: "lb_openshift_{{ openshift_cluster_name }}_api_ignition"
          servicetype: SSL_BRIDGE
          ipaddress: "{{ openshift_api_ip }}"
          port: "{{ openshift_api_ignition_port | int }}"
          servicegroup:
            name: "lb_svg_openshift_{{ openshift_cluster_name }}_api_ignition"
            servicetype: SSL_BRIDGE
            mode: exact
            members: "{{ temp_servicegroup_members_api_ignition }}"

    - name: Build load balancer topology - Create list
      set_fact:
        citrix_vservers: "{{ citrix_vservers_api + citrix_vservers_api_ignition }}"

    - name: Deploy Service Groups
      ansible.builtin.import_tasks: tasks/deploy-citrix-servicegroups.yml


- name: Finalize OpenShift deployment
  hosts: openshift_bootstrap_nodes
  gather_facts: false
  vars_files:
  - passwords/ansible_vault.yml
  - vars/network-config.yml
  - group_vars/vcenter_vms.yml
  environment:
    KUBECONFIG: "{{ vars.openshift_config_dir }}/auth/kubeconfig"
  tasks:

  - name: Deploy cluster if the namespace 'openshift-kube-apiserver' doesn't exist
    when: vars.deploy_bootstrap_results.stderr_lines is defined and '"level=info msg=It is now safe to remove the bootstrap resources" in vars.deploy_bootstrap_results.stderr_lines'
    tags: deploy_cluster
    delegate_to: localhost
    block:

    - name: Check etcd members
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Etcd
      register: etcd_list

    - name: Print found Etcd members
      debug:
        msg: "{{ item.nodeName }}"
      with_items: "{{ etcd_list.resources[0].status.nodeStatuses }}"
      loop_control:
        label: "Etcd member hostname"

    # - name: Accept worker node bootstrap & node serving CSRs
    #   ansible.builtin.include_tasks: tasks/deploy_openshift_accept_csr.yml
    #   loop: "{{ range(2) }}"

    - name: 'Set # of nodes to wait for'
      set_fact:
        number_of_ready_nodes: "{{ 0 +
          hostvars_dict.values()
            | selectattr('openshift_cluster_name', 'defined')
            | selectattr('openshift_cluster_name', 'eq', vars.openshift_cluster_name)
            | selectattr('type', 'eq', 'master')
            | map(attribute='ip')
            | list | length | int +
          hostvars_dict.values()
            | selectattr('openshift_cluster_name', 'defined')
            | selectattr('openshift_cluster_name', 'eq', vars.openshift_cluster_name)
            | selectattr('type', 'eq', 'worker')
            | map(attribute='ip')
            | list | length | int }}"

    - name: Accept worker node bootstrap & node serving CSRs
      ansible.builtin.include_tasks: tasks/deploy_openshift_accept_csr_till_all_nodes_ready.yml


    - name: Get 'cloud-provider-config' ConfigMap to specify a custom VM Folder
      kubernetes.core.k8s_info:
        api_version: v1
        kind: ConfigMap
        name: cloud-provider-config
        namespace: openshift-config
      register: cloud_provider_config

    - name: Set 'cloud-provider-config' ConfigMap to correct VM folder
      kubernetes.core.k8s:
        api_version: v1
        definition:
          api_version: v1
          kind: ConfigMap
          metadata:
            name: cloud-provider-config
            namespace: openshift-config 
          data:
            config: "{{ cloud_provider_config.resources[0].data.config | regex_replace ('folder = (.*)', 'folder = \"/' +
                          default_community_vmware_vmware__guest.datacenter + '/vm' + default_community_vmware_vmware__guest.folder + '\"') }}"

 
    - name: Add PVC for 'image-registry' ClusterOperator
      kubernetes.core.k8s:
        api_version: v1
        definition:
          api_version: v1
          kind: PersistentVolumeClaim
          metadata:
            name: image-registry-storage
            namespace: openshift-image-registry 
          spec:
            accessModes:
            - ReadWriteOnce 
            resources:
              requests:
                storage: 100Gi 

    - name: Set 'image-registry' ClusterOperator config to recreate itself
      kubernetes.core.k8s:
        api_version: imageregistry.operator.openshift.io/v1
        definition:
          api_version: imageregistry.operator.openshift.io/v1
          kind: Config
          metadata:
            name: cluster
          spec:
            managementState: Managed
            rolloutStrategy: Recreate
            replicas: 1
            storage:
              pvc:
                claim: image-registry-storage
      register: image_registry_config

    - name: If image-registry was changed, wait 5 minutes before proceeding so the operator can reconfigure
      pause:
        minutes: 5
      when: image_registry_config.changed


    - name: Start install wait-for install-complete (40 minute time-out each retry)
      shell: "{{ vars.openshift_download_dir }}/openshift-install --dir {{ vars.openshift_config_dir }} wait-for install-complete --log-level=debug"
      register: deploy_install_results
      retries: 2
      delay: 5
      until: '"level=info msg=Install complete!" in deploy_install_results.stderr_lines'
      ignore_errors: yes

    - name: Fail / halt if there is an error with command parameters
      fail:
        msg: "Error with command parameters, help is shown"
      when: '"Wait for install-time events." in deploy_install_results.stdout_lines'

    - name: Show wait-for install-complete output
      debug:
        msg: "{{ ['Command executed in ' + deploy_install_results.attempts | string + ' tries, output:',
                  '-----------------------------------'] +
                  deploy_install_results.stderr_lines }}"


    - name: List Nodes
      shell: oc get nodes
      delegate_to: localhost
      register: cli_nodes_results

    - debug:
        msg: "{{ ['Nodes:'] + cli_nodes_results.stdout_lines }}"

    - name: List ClusterOperators
      shell: oc get co
      delegate_to: localhost
      register: cli_co_results

    - debug:
        msg: "{{ ['ClusterOperators:'] + cli_co_results.stdout_lines }}"

    - name: List ClusterVersion
      shell: oc get clusterversion
      delegate_to: localhost
      register: cli_cv_results

    - debug:
        msg: "{{ ['ClusterVersion:'] + cli_cv_results.stdout_lines }}"



- name: Power on infra nodes
  hosts: openshift_infra_nodes
  gather_facts: false
  vars_files:
  - passwords/ansible_vault.yml
  - vars/network-config.yml
  tasks:

  - name: Start the infra nodes
    tags: deploy_cluster_infra
    block:

    - name: Power on infra nodes
      retries: 3
      delay: 10
      community.vmware.vmware_guest:
        validate_certs: "{{ validate_certs | default(false) }}"
        hostname: "{{ vcenter_hostname | default(lookup('env', 'VMWARE_HOST')) }}"
        username: "{{ vcenter_username | default(lookup('env', 'VMWARE_USER')) }}"
        password: "{{ vcenter_password | default(lookup('env', 'VMWARE_PASSWORD')) }}"
        name: "{{ inventory_hostname_short }}"
        state: poweredon
      delegate_to: localhost
      register: infra_node_boot

    - name: If booted, wait 5 minutes before proceeding so the node can bootstrap
      pause:
        minutes: 5
      when: infra_node_boot.changed


- name: Add infra nodes to load balancers
  hosts: openshift_bootstrap_nodes
  gather_facts: false
  vars_files:
  - passwords/ansible_vault.yml
  - vars/network-config.yml
  - group_vars/citrix_load_balancer.yml
  tags: prep_citrix
  tasks:

  - name: Add infra nodes to load balancers
    when: vars.deploy_bootstrap_results.stderr_lines is defined and '"level=info msg=It is now safe to remove the bootstrap resources" in vars.deploy_bootstrap_results.stderr_lines'
    # tags: deploy_cluster
    delegate_to: localhost
    block:

    - name: Reset Apps service group
      set_fact:
        temp_servicegroup_members_apps: []

    - name: Build load balancer topology - Apps service group
      set_fact:
        temp_servicegroup_members_apps: '{{ temp_servicegroup_members_apps + [{
          "ip": hostvars[item].ip,
          "port": 65535,
          "weight": 1
        }] }}'
      when: hostvars[item].openshift_cluster_name == vars.openshift_cluster_name
      with_items:
      - "{{ groups.openshift_worker_nodes }}"
      - "{{ groups.openshift_infra_nodes }}"

    - name: Build load balancer topology - API load balancer
      set_fact:
        citrix_vservers_apps:
        - name: "lb_openshift_{{ openshift_cluster_name }}_apps_any"
          servicetype: ANY
          ipaddress: "{{ openshift_ingress_ip }}"
          port: 65535
          servicegroup:
            name: "lb_svg_openshift_{{ openshift_cluster_name }}_apps"
            servicetype: ANY
            mode: exact
            members: "{{ temp_servicegroup_members_apps }}"

    - name: Build load balancer topology - Create list
      set_fact:
        citrix_vservers: "{{ citrix_vservers_apps }}"

    - name: Deploy Service Groups
      ansible.builtin.import_tasks: tasks/deploy-citrix-servicegroups.yml


- name: Deploy dedicated OpenShift infra nodes
  hosts: openshift_bootstrap_nodes
  gather_facts: false
  vars_files:
  - passwords/ansible_vault.yml
  - vars/network-config.yml
  - group_vars/vcenter_vms.yml
  environment:
    KUBECONFIG: "{{ vars.openshift_config_dir }}/auth/kubeconfig"
  tasks:

  - name: Deploy infra nodes
    tags: deploy_cluster_infra
    delegate_to: localhost
    block:

    - name: 'Set # of nodes to wait for'
      set_fact:
        number_of_ready_nodes: "{{ 0 +
          hostvars_dict.values()
            | selectattr('openshift_cluster_name', 'defined')
            | selectattr('openshift_cluster_name', 'eq', vars.openshift_cluster_name)
            | selectattr('type', 'eq', 'master')
            | map(attribute='ip')
            | list | length | int +
          hostvars_dict.values()
            | selectattr('openshift_cluster_name', 'defined')
            | selectattr('openshift_cluster_name', 'eq', vars.openshift_cluster_name)
            | selectattr('type', 'eq', 'worker')
            | map(attribute='ip')
            | list | length | int +
          hostvars_dict.values()
            | selectattr('openshift_cluster_name', 'defined')
            | selectattr('openshift_cluster_name', 'eq', vars.openshift_cluster_name)
            | selectattr('type', 'eq', 'infra')
            | map(attribute='ip')
            | list | length | int }}"

    - name: Accept infra node bootstrap & node serving CSRs
      ansible.builtin.include_tasks: tasks/deploy_openshift_accept_csr_till_all_nodes_ready.yml

    - name: Mark infra nodes unschedulable with a proper taint
      kubernetes.core.k8s:
        api_version: v1
        definition:
          api_version: v1
          kind: Node
          metadata:
            name: "{{ hostvars[item].inventory_hostname_short }}"
            labels:
              node-role.kubernetes.io/infra: ""
          spec:
            taints:
            - effect: NoSchedule
              key: node-role.kubernetes.io/infra
      when: hostvars[item].openshift_cluster_name == vars.openshift_cluster_name
      with_items: "{{ groups.openshift_infra_nodes }}"


    - name: Deploy a new default router definition allowing scheduling on infra nodes only, as json to prevent issues on number of replicas not being an int
      kubernetes.core.k8s:
        state: present
        template: 'templates/openshift_4_router-default.yml'


    - name: Allow taint for 'image-registry' ClusterOperator to run on infra nodes
      kubernetes.core.k8s:
        api_version: imageregistry.operator.openshift.io/v1
        definition:
          api_version: imageregistry.operator.openshift.io/v1
          kind: Config
          metadata:
            name: cluster
          spec:
            tolerations:
            - effect: NoSchedule
              key: node-role.kubernetes.io/infra
              operator: "Exists"
            nodeSelector:
              node-role.kubernetes.io/infra: ""
            affinity:
              podAffinity: 
                preferredDuringSchedulingIgnoredDuringExecution:
                - weight: 100
                  podAffinityTerm:
                    topologyKey: "kubernetes.io/hostname"
                    namespaces:
                    - "openshift-image-registry"


    - name: Deploy a new monitoring ConfigMap definition allowing scheduling on infra nodes only
      kubernetes.core.k8s:
        state: present
        template: 'templates/openshift_4_cluster-monitoring-config.yml'
      register: monitoring_configmap

    - name: If image-registry was changed, wait 5 minutes before proceeding so the operator can reconfigure
      pause:
        minutes: 5
      when: monitoring_configmap.changed


    - name: List Nodes
      shell: oc get nodes
      register: cli_nodes_results

    - debug:
        msg: "{{ ['Nodes:'] + cli_nodes_results.stdout_lines }}"

    - name: List ClusterOperators
      shell: oc get co
      register: cli_co_results

    - debug:
        msg: "{{ ['ClusterOperators:'] + cli_co_results.stdout_lines }}"


- name: Remove worker nodes from load balancers
  hosts: openshift_bootstrap_nodes
  gather_facts: false
  vars_files:
  - passwords/ansible_vault.yml
  - vars/network-config.yml
  - group_vars/citrix_load_balancer.yml
  tags: prep_citrix
  tasks:

  - name: Remove worker nodes from load balancers
    when: vars.deploy_bootstrap_results.stderr_lines is defined and '"level=info msg=It is now safe to remove the bootstrap resources" in vars.deploy_bootstrap_results.stderr_lines'
    tags: deploy_cluster
    delegate_to: localhost
    block:

    - name: Reset Apps service group
      set_fact:
        temp_servicegroup_members_apps: []

    - name: Build load balancer topology - Apps service group
      set_fact:
        temp_servicegroup_members_apps: '{{ temp_servicegroup_members_apps + [{
          "ip": hostvars[item].ip,
          "port": 65535,
          "weight": 1
        }] }}'
      when: hostvars[item].openshift_cluster_name == vars.openshift_cluster_name
      with_items:
      - "{{ groups.openshift_infra_nodes }}"

    - name: Reset load balancer
      set_fact:
        citrix_vservers_apps: []

    - name: Build load balancer topology - API load balancer
      set_fact:
        citrix_vservers_apps:
        - name: "lb_openshift_{{ openshift_cluster_name }}_apps_any"
          servicetype: ANY
          ipaddress: "{{ openshift_ingress_ip }}"
          port: 65535
          servicegroup:
            name: "lb_svg_openshift_{{ openshift_cluster_name }}_apps"
            servicetype: ANY
            mode: exact
            members: "{{ temp_servicegroup_members_apps }}"

    - name: Build load balancer topology - Create list
      set_fact:
        citrix_vservers: "{{ citrix_vservers_apps }}"

    - name: Deploy Service Groups
      ansible.builtin.import_tasks: tasks/deploy-citrix-servicegroups.yml


  # ToDo: Creating a separate /var partition
  # ToDo: Updating the bootloader using bootupd
  # ToDo: test MachineNetwork in 10.255.254.0/24
  # ToDo: Logging

