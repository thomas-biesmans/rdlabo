--- 
- import_playbook: tasks/check_all.yml


- name: Check OpenShift deployment status
  hosts: localhost
  gather_facts: false
  vars_files:
  - passwords/ansible_vault.yml
  - vars/network-config.yml
  - group_vars/openshift_upi_vms.yml
  - group_vars/vcenter_vms.yml
  environment:
    KUBECONFIG: "{{ vars.openshift_config_dir }}/auth/kubeconfig"
  tasks:

  - name: Check whether OpenShift cluster exists already
    tags: deploy_cluster
    block:

    - name: Check whether OpenShift cluster exists already
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Namespace
        name: openshift-kube-apiserver
      register: namespace_kube_apiserver
      timeout: 10
      retries: 3
      ignore_errors: yes

    - name: "Log when namespace 'openshift-kube-apiserver' already exists"
      debug:
        msg: "Namespace 'openshift-kube-apiserver' already exists, no need to redeploy."
      when: not namespace_kube_apiserver.failed and namespace_kube_apiserver.resources[0].status.phase | default('') == "Active"


- name: Deploy VMs
  hosts: openshift_upi_vms # openshift_upi_vms_master_nodes[0]:openshift_upi_vms_worker_nodes[1]
  gather_facts: false
  vars_files:
  - passwords/ansible_vault.yml
  - vars/network-config.yml
  tasks:

  - name: Combine customized VM config
    set_fact:
      community_vmware_vmware__guest: "{{ default_community_vmware_vmware__guest | combine(custom_community_vmware_vmware__guest | default({}), recursive=true) }}"
      ansible_becomes_password: "{{ client_password }}"
    tags: always

  # - name: Ensure Ansible become password is set
  #   set_fact:
  #     ansible_become_password: "{{ deployment_password }}"
  #   when: ansible_become_password is undefined or ansible_become_password != deployment_password
  #   tags: always

  # - name: biep
  #   debug:
  #     msg: "{{ vars.community_vmware_vmware__guest }}"


  - name: Deploy base infrastructure
    when: hostvars.localhost.namespace_kube_apiserver.failed or hostvars.localhost.namespace_kube_apiserver.resources[0].status.phase != "Active"
    tags: deploy_infrastructure
    block:

    - name: Deploy DNS records
      vars:
        target_group_dns: "{{ ansible_play_hosts }}" #"{{ groups.rhel_vms }}"
      ansible.builtin.import_tasks: tasks/deploy-windows-dns-records.yml

    - name: Deploy DHCP reservations
      vars:
        target_group_dns: "{{ ansible_play_hosts }}" #"{{ groups.rhel_vms }}"
      ansible.builtin.import_tasks: tasks/deploy-windows-dhcp-reservations.yml

    - name: Deploy VMs
      ansible.builtin.include_tasks:
        file: tasks/deploy-vmware-vms-from-template.yml
        apply:
          tags: vm
      tags: vm


- name: Prepare the web server
  hosts: localhost
  gather_facts: false
  become: true
  vars_files:
  - passwords/ansible_vault.yml
  - vars/network-config.yml
  - group_vars/openshift_upi_vms.yml
  - group_vars/vcenter_vms.yml
  tasks:

  - name: Prepare bootstrap environment
    when: hostvars.localhost.namespace_kube_apiserver.failed or hostvars.localhost.namespace_kube_apiserver.resources[0].status.phase != "Active"
    tags: prep_bootstrap
    block:

    - name: Install podman
      ansible.builtin.yum:
        name: podman
        state: latest
      tags: web

    - name: Allow traffic to httpd on port 8080
      ansible.posix.firewalld:
        zone: public
        port: 8080/tcp
        permanent: yes
        state: enabled
      tags: web
      vars:
        ansible_python_interpreter: /usr/bin/python3.6


- name: Prepare OpenShift deployment
  hosts: localhost
  gather_facts: false
  vars_files:
  - passwords/ansible_vault.yml
  - vars/network-config.yml
  - group_vars/openshift_upi_vms.yml
  - group_vars/vcenter_vms.yml
  tasks:


  - name: Prepare bootstrap environment
    when: hostvars.localhost.namespace_kube_apiserver.failed or hostvars.localhost.namespace_kube_apiserver.resources[0].status.phase != "Active"
    tags: prep_bootstrap
    block:

    - name: OpenShift deployment preparation
      tags: openshift
      block:

      - name: Create config & install dir
        file:
          path: "{{ item }}"
          state: directory
        with_items:
        - "{{ vars.openshift_config_dir }}"
        - "{{ vars.openshift_download_dir }}"


      - name: Download specific sources
        get_url:
          url: "{{ vars.openshift_custom_version_source }}"
          dest: "{{ vars.openshift_download_dir }}/openshift-install-linux.tar.gz"
        when: vars.openshift_custom_version_source | length > 0

      - name: Download default stable sources if no specific version is set
        get_url:
          url: "{{ vars.openshift_default_source }}"
          dest: "{{ vars.openshift_download_dir }}/openshift-install-linux.tar.gz"
        when: vars.openshift_custom_version_source | length <= 0
      
      - name: Untar the installation file
        unarchive:
          src: "{{ vars.openshift_download_dir }}/openshift-install-linux.tar.gz"
          dest: "{{ vars.openshift_download_dir }}"
          creates: "{{ vars.openshift_download_dir }}/openshift-install"


      - name: Download default stable client
        get_url:
          url: "{{ vars.openshift_default_client_source }}"
          dest: "{{ vars.openshift_download_dir }}/openshift-client-linux.tar.gz"
      
      - name: Untar the client file
        unarchive:
          src: "{{ vars.openshift_download_dir }}/openshift-client-linux.tar.gz"
          dest: "{{ vars.openshift_download_dir }}"
          creates: "{{ vars.openshift_download_dir }}/oc"
      
      # - name: Copy client files to the local user's bin, instead of /usr/local/bin which would need become
      #   copy:
      #     src: "{{ item }}"
      #     dest: $HOME/bin/
      #   with_items: 
      #   - "{{ vars.openshift_download_dir }}/oc"
      #   - "{{ vars.openshift_download_dir }}/kubectl"

      - name: Copy client files to /usr/local/bin for all users
        copy:
          src: "{{ item }}"
          dest: /usr/local/bin
        with_items: 
        - "{{ vars.openshift_download_dir }}/oc"
        - "{{ vars.openshift_download_dir }}/kubectl"

      - name: Check whether new SSH key is already created
        include_tasks: tasks/check_local_ssh_key_creation.yml
        run_once: true

      - name: Read SSH key
        slurp:
          src: "~/.ssh/{{ vars.deployment_ssh_filename }}.pub"
        register: sshKey
        
      - name: Check whether manifests were already created
        stat:
          path: "{{ vars.openshift_config_dir }}/manifests"
        register: manifests_dir

      - name: Check whether ignition files were already created
        stat:
          path: "{{ vars.openshift_config_dir }}/master.ign"
        register: ignition_file

      - name: Create manifests
        when: not manifests_dir.stat.exists and not ignition_file.stat.exists
        block:

        - name: Template out the inventory file to the config dir
          template:
            src: templates/openshift_4_upi_install_config_vmware_template.yml
            dest: "{{ vars.openshift_config_dir }}/install-config.yaml"
            force: no

        - name: Take a copy of the install-config as a backup
          copy:
            src: "{{ vars.openshift_config_dir }}/install-config.yaml"
            dest: "{{ vars.openshift_config_dir }}/install-config.yaml.bak"
            force: no

        - name: Create manifest files
          shell: "{{ vars.openshift_download_dir }}/openshift-install create manifests --dir {{ vars.openshift_config_dir }}"
    
        - name: Remove unneeded manifest files
          file:
            path: "{{ item }}"
            state: absent
          with_fileglob:
          - "{{ vars.openshift_config_dir }}/openshift/99_openshift-cluster-api_master-machines-*.yaml"
          - "{{ vars.openshift_config_dir }}/openshift/99_openshift-cluster-api_worker-machineset-*.yaml"
          
        - name: Set masters to non-Schedulable
          replace:
            path: "{{ vars.openshift_config_dir }}/manifests/cluster-scheduler-02-config.yml"
            regexp: 'mastersSchedulable: true'
            replace: 'mastersSchedulable: false'


      - name: Create ignition files
        shell: "{{ vars.openshift_download_dir }}/openshift-install create ignition-configs --dir {{ vars.openshift_config_dir }}"
        when: not ignition_file.stat.exists
  
      - name: Query the cluster's random identifier
        shell: "jq -r .infraID {{ vars.openshift_config_dir }}/metadata.json"
        register: cluster_id

      - name: Report cluster name
        debug:
          msg: "Cluster ID: {{ cluster_id.stdout }}"


    - name: Configure a web server for the ignition files
      tags:
      - ignition_prep
      block:
    
        - name: Pull Apache httpd container image
          containers.podman.podman_image:
            name: httpd
            pull: true
            tag: latest
    
        - name: Configure webroot - permissions
          file:
            path: "{{ vars.bastion_webroot }}"
            state: directory
            mode: '0777'

        - name: Configure webroot - SELinux file context, should be idempotent, isn't...
          become: yes
          community.general.sefcontext:
            target: "{{ vars.bastion_webroot }}(/.*)?"
            setype: "container_share_t"
            state: present
          register: webroot_dir_fcontext
          vars:
            ansible_python_interpreter: /usr/bin/python3.6

        - name: Create a custom index.html
          copy:
            dest: "{{ vars.bastion_webroot }}/index.html"
            content: |
              Custom Web Page, hosting ignition files
    
        - name: Copy the ignition file
          copy:
            src: "{{ vars.openshift_config_dir }}/{{ item }}.ign"
            dest: "{{ vars.bastion_webroot }}/{{ item }}.ign"
          with_items:
          - "master"
          - "worker"
          - "bootstrap"
    
        - name: Apply SELinux fcontext if it wasn't already added
          become: yes
          shell: restorecon -irv {{ vars.bastion_webroot }}
          when: webroot_dir_fcontext.changed
    
        - name: Run Apache httpd container
          containers.podman.podman_container:
            name: webserver
            image: httpd
            state: started
            detach: true
            expose:
            - 80
            ports:
            - "{{ vars.bastion_port }}:80"
            volume:
            - "{{ vars.bastion_webroot }}:/usr/local/apache2/htdocs/:exec"

        - name: Query index.html
          shell: "curl http://{{ vars.bastion_ip }}:{{ vars.bastion_port }}/index.html"
          register: index_curl

        - name: Show output received from web server
          debug:
            msg: "Output index.html: {{ index_curl.stdout }}"

        - name: Query bootstrap ignition
          shell: "curl http://{{ vars.bastion_ip }}:{{ vars.bastion_port }}/bootstrap.ign"
          register: bootstrap_curl

        - name: Show output received from web server
          debug:
            msg: "Output bootstrap.ign: {{ bootstrap_curl.stdout[:12] }}"

        - name: Stop if ignition config isn't available
          fail:
          when: bootstrap_curl.stdout[:12] != "{\"ignition\":"

        - name: Template out the inventory file to the config dir
          template:
            src: templates/openshift_4_upi_ignition_template.yml
            dest: "{{ vars.openshift_config_dir }}/{{ item }}.ign"
            force: no
          with_items:
          - "merge-bootstrap"

        - name: Encode them in Base64
          # shell: "base64 -w0 > {{ vars.openshift_config_dir }}/{{ item }}.64"
          copy:
            content: "{{ lookup('file', '{{ vars.openshift_config_dir }}/{{ item }}.ign') | b64encode }}"
            dest: "{{ vars.openshift_config_dir }}/{{ item }}.64"
          with_items:
          - "master"
          - "worker"
          - "merge-bootstrap"


- name: Deploy ignition configs on masters
  hosts: openshift_upi_vms_master_nodes
  gather_facts: false
  vars_files:
  - passwords/ansible_vault.yml
  - vars/network-config.yml
  tasks:


  - name: Prepare nodes
    when: hostvars.localhost.namespace_kube_apiserver.failed or hostvars.localhost.namespace_kube_apiserver.resources[0].status.phase != "Active"
    tags: prep_nodes
    block:

    - name: Deploy ignition configs
      tags: ignition_vm_deploy
      block:

      - name: Update VMs' Advanced Settings with Base64 encoded ignition data
        throttle: "{{ parallel_deployments }}"
        retries: 3
        delay: 10
        community.vmware.vmware_guest:
          validate_certs: "{{ validate_certs | default(false) }}"
          hostname: "{{ vcenter_hostname | default(lookup('env', 'VMWARE_HOST')) }}"
          username: "{{ vcenter_username | default(lookup('env', 'VMWARE_USER')) }}"
          password: "{{ vcenter_password | default(lookup('env', 'VMWARE_PASSWORD')) }}"
          name: "{{ inventory_hostname_short }}"
          state: present
          advanced_settings:
          - key: guestinfo.ignition.config.data
            value: "{{ lookup('file', '{{ vars.openshift_config_dir }}/{{ item }}.ign') | b64encode }}"

        delegate_to: localhost
        with_items:
        - "master"

- name: Deploy ignition configs on workers
  hosts: openshift_upi_vms_worker_nodes, openshift_upi_vms_infra_nodes
  gather_facts: false
  vars_files:
  - passwords/ansible_vault.yml
  - vars/network-config.yml
  tasks:

  - name: Prepare nodes
    when: hostvars.localhost.namespace_kube_apiserver.failed or hostvars.localhost.namespace_kube_apiserver.resources[0].status.phase != "Active"
    tags: prep_nodes
    block:

    - name: Update VMs' Advanced Settings with Base64 encoded ignition data
      throttle: "{{ parallel_deployments }}"
      retries: 3
      delay: 10
      community.vmware.vmware_guest:
        validate_certs: "{{ validate_certs | default(false) }}"
        hostname: "{{ vcenter_hostname | default(lookup('env', 'VMWARE_HOST')) }}"
        username: "{{ vcenter_username | default(lookup('env', 'VMWARE_USER')) }}"
        password: "{{ vcenter_password | default(lookup('env', 'VMWARE_PASSWORD')) }}"
        name: "{{ inventory_hostname_short }}"
        state: present
        advanced_settings:
        - key: guestinfo.ignition.config.data
          value: "{{ lookup('file', '{{ vars.openshift_config_dir }}/{{ item }}.ign') | b64encode }}"

      delegate_to: localhost
      tags: ignition_vm_deploy
      with_items:
      - "worker"


- name: Deploy ignition configs on bootstrap node
  hosts: openshift_upi_vms_bootstrap_node
  gather_facts: false
  vars_files:
  - passwords/ansible_vault.yml
  - vars/network-config.yml
  tasks:

  - name: Prepare nodes
    when: hostvars.localhost.namespace_kube_apiserver.failed or hostvars.localhost.namespace_kube_apiserver.resources[0].status.phase != "Active"
    tags: prep_nodes
    block:

    - name: Update VMs' Advanced Settings with Base64 encoded ignition data
      throttle: "{{ parallel_deployments }}"
      retries: 3
      delay: 10
      community.vmware.vmware_guest:
        validate_certs: "{{ validate_certs | default(false) }}"
        hostname: "{{ vcenter_hostname | default(lookup('env', 'VMWARE_HOST')) }}"
        username: "{{ vcenter_username | default(lookup('env', 'VMWARE_USER')) }}"
        password: "{{ vcenter_password | default(lookup('env', 'VMWARE_PASSWORD')) }}"
        name: "{{ inventory_hostname_short }}"
        state: present
        advanced_settings:
        - key: guestinfo.ignition.config.data
          value: "{{ lookup('file', '{{ vars.openshift_config_dir }}/{{ item }}.ign') | b64encode }}"

      delegate_to: localhost
      tags: ignition_vm_deploy
      with_items:
      - "merge-bootstrap"



- name: Power on bootstrap node if bootstrap is needed
  hosts: openshift_upi_vms_bootstrap_node
  gather_facts: false
  vars_files:
  - passwords/ansible_vault.yml
  - vars/network-config.yml
  tasks:

  - name: Start the bootstrap node
    when: hostvars.localhost.namespace_kube_apiserver.failed or hostvars.localhost.namespace_kube_apiserver.resources[0].status.phase != "Active"
    tags: deploy_cluster
    block:

    - name: Power on bootstrap node if bootstrap is needed
      retries: 3
      delay: 10
      community.vmware.vmware_guest:
        validate_certs: "{{ validate_certs | default(false) }}"
        hostname: "{{ vcenter_hostname | default(lookup('env', 'VMWARE_HOST')) }}"
        username: "{{ vcenter_username | default(lookup('env', 'VMWARE_USER')) }}"
        password: "{{ vcenter_password | default(lookup('env', 'VMWARE_PASSWORD')) }}"
        name: "{{ inventory_hostname_short }}"
        state: poweredon
      delegate_to: localhost

    - name: Query ignition files served by the bootstrap node, waiting up to 5 minutes
      shell: "curl -k https://{{ openshift_api_url }}:22623/config/master"
      retries: 10
      delay: 30
      until: ignition_curl.stdout[:20] == "{\"ignition\":{\"versio"
      register: ignition_curl
      delegate_to: localhost

    - name: Show output received from bootstrapped cluster
      debug:
        msg: "Output: {{ ignition_curl.stdout[:20] }}"


- name: Power on master nodes if bootstrap is needed
  hosts: openshift_upi_vms_master_nodes
  gather_facts: false
  vars_files:
  - passwords/ansible_vault.yml
  - vars/network-config.yml
  tasks:

  - name: Start the master nodes if bootstrap is needed
    when: hostvars.localhost.namespace_kube_apiserver.failed or hostvars.localhost.namespace_kube_apiserver.resources[0].status.phase != "Active"
    tags: deploy_cluster
    block:

    - name: Power on master nodes if bootstrap is needed
      retries: 3
      delay: 10
      community.vmware.vmware_guest:
        validate_certs: "{{ validate_certs | default(false) }}"
        hostname: "{{ vcenter_hostname | default(lookup('env', 'VMWARE_HOST')) }}"
        username: "{{ vcenter_username | default(lookup('env', 'VMWARE_USER')) }}"
        password: "{{ vcenter_password | default(lookup('env', 'VMWARE_PASSWORD')) }}"
        name: "{{ inventory_hostname_short }}"
        state: poweredon
      delegate_to: localhost


- name: Power on worker nodes if bootstrap is needed
  hosts: openshift_upi_vms_worker_nodes
  gather_facts: false
  vars_files:
  - passwords/ansible_vault.yml
  - vars/network-config.yml
  tasks:

  - name: Start the worker nodes if bootstrap is needed
    when: hostvars.localhost.namespace_kube_apiserver.failed or hostvars.localhost.namespace_kube_apiserver.resources[0].status.phase != "Active"
    tags: deploy_cluster
    block:

    - name: Power on worker nodes if bootstrap is needed
      retries: 3
      delay: 10
      community.vmware.vmware_guest:
        validate_certs: "{{ validate_certs | default(false) }}"
        hostname: "{{ vcenter_hostname | default(lookup('env', 'VMWARE_HOST')) }}"
        username: "{{ vcenter_username | default(lookup('env', 'VMWARE_USER')) }}"
        password: "{{ vcenter_password | default(lookup('env', 'VMWARE_PASSWORD')) }}"
        name: "{{ inventory_hostname_short }}"
        state: poweredon
      delegate_to: localhost


- name: Deploy OpenShift
  hosts: localhost
  gather_facts: false
  vars_files:
  - passwords/ansible_vault.yml
  - vars/network-config.yml
  - group_vars/openshift_upi_vms.yml
  - group_vars/vcenter_vms.yml
  environment:
    KUBECONFIG: "{{ vars.openshift_config_dir }}/auth/kubeconfig"
  tasks:

  - name: Deploy cluster 
    tags: deploy_cluster
    block:

    - name: Start install wait-for bootstrap-complete (30 minute time-out each retry)
      shell: "{{ vars.openshift_download_dir }}/openshift-install --dir {{ vars.openshift_config_dir }} wait-for bootstrap-complete --log-level=debug"
      register: deploy_bootstrap_results
      retries: 2
      delay: 5
      until: '"level=info msg=It is now safe to remove the bootstrap resources" in deploy_bootstrap_results.stderr_lines'
      ignore_errors: yes

    - name: Fail / halt if there is an error with command parameters
      fail:
        msg: "Error with command parameters, help is shown"
      when: '"Wait for install-time events." in deploy_bootstrap_results.stdout_lines'

    - name: Fail / halt if bootstrap didn't complete
      fail:
        msg: ["Error with command, you can gather bootstrap information with the following command:",
              "{{ vars.openshift_download_dir }}/openshift-install --dir {{ vars.openshift_config_dir }} gather bootstrap
                --bootstrap {{ hostvars[groups.openshift_upi_vms_bootstrap_node[0]].ip }}
                --master {{ hostvars[groups.openshift_upi_vms_master_nodes[0]].ip }}
                --master {{ hostvars[groups.openshift_upi_vms_master_nodes[1]].ip }}
                --master {{ hostvars[groups.openshift_upi_vms_master_nodes[2]].ip }}
              "]
      when: 'not "level=info msg=It is now safe to remove the bootstrap resources" in deploy_bootstrap_results.stderr_lines'

    - name: Show wait-for bootstrap-complete output
      debug:
        msg: "{{ ['Command executed in ' + deploy_bootstrap_results.attempts | string + ' tries, output:',
                  '-----------------------------------'] +
                  deploy_bootstrap_results.stderr_lines }}"


- name: Power off bootstrap node if bootstrap is finished
  hosts: openshift_upi_vms_bootstrap_node
  gather_facts: false
  vars_files:
  - passwords/ansible_vault.yml
  - vars/network-config.yml
  tasks:

  - name: Power off the bootstrap node
    when: '"level=info msg=It is now safe to remove the bootstrap resources" in hostvars.localhost.deploy_bootstrap_results.stderr_lines'
    tags: deploy_cluster
    block:

    - name: Power off bootstrap node if bootstrap is finished
      retries: 3
      delay: 10
      community.vmware.vmware_guest:
        validate_certs: "{{ validate_certs | default(false) }}"
        hostname: "{{ vcenter_hostname | default(lookup('env', 'VMWARE_HOST')) }}"
        username: "{{ vcenter_username | default(lookup('env', 'VMWARE_USER')) }}"
        password: "{{ vcenter_password | default(lookup('env', 'VMWARE_PASSWORD')) }}"
        name: "{{ inventory_hostname_short }}"
        state: poweredoff
      delegate_to: localhost


- name: Finalize OpenShift deployment
  hosts: localhost
  gather_facts: false
  vars_files:
  - passwords/ansible_vault.yml
  - vars/network-config.yml
  - group_vars/openshift_upi_vms.yml
  - group_vars/vcenter_vms.yml
  environment:
    KUBECONFIG: "{{ vars.openshift_config_dir }}/auth/kubeconfig"
  tasks:

  - name: Deploy cluster if the namespace 'openshift-kube-apiserver' doesn't exist
    when: '"level=info msg=It is now safe to remove the bootstrap resources" in hostvars.localhost.deploy_bootstrap_results.stderr_lines'
    tags: deploy_cluster
    block:

    - name: Check etcd members
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Etcd
      register: etcd_list

    - name: Print found Etcd members
      debug:
        msg: "{{ item.nodeName }}"
      with_items: "{{ etcd_list.resources[0].status.nodeStatuses }}"
      loop_control:
        label: "Etcd member hostname"

    # - name: Accept worker node bootstrap & node serving CSRs
    #   ansible.builtin.include_tasks: tasks/deploy_openshift_accept_csr.yml
    #   loop: "{{ range(2) }}"

    - name: 'Set # of nodes to wait for'
      set_fact:
        number_of_ready_nodes: "{{ 0 + groups.openshift_upi_vms_master_nodes | length | int + groups.openshift_upi_vms_worker_nodes | length | int }}"

    - name: Accept worker node bootstrap & node serving CSRs
      ansible.builtin.include_tasks: tasks/deploy_openshift_accept_csr_till_all_nodes_ready.yml


    - name: Get 'cloud-provider-config' ConfigMap to specify a custom VM Folder
      kubernetes.core.k8s_info:
        api_version: v1
        kind: ConfigMap
        name: cloud-provider-config
        namespace: openshift-config
      register: cloud_provider_config

    - name: Set 'cloud-provider-config' ConfigMap to correct VM folder
      kubernetes.core.k8s:
        api_version: v1
        definition:
          api_version: v1
          kind: ConfigMap
          metadata:
            name: cloud-provider-config
            namespace: openshift-config 
          data:
            config: "{{ cloud_provider_config.resources[0].data.config | regex_replace ('folder = (.*)', 'folder = \"/' +
                          default_community_vmware_vmware__guest.datacenter + '/vm' + default_community_vmware_vmware__guest.folder + '\"') }}"

 
    - name: Add PVC for 'image-registry' ClusterOperator
      kubernetes.core.k8s:
        api_version: v1
        definition:
          api_version: v1
          kind: PersistentVolumeClaim
          metadata:
            name: image-registry-storage
            namespace: openshift-image-registry 
          spec:
            accessModes:
            - ReadWriteOnce 
            resources:
              requests:
                storage: 100Gi 

    - name: Set 'image-registry' ClusterOperator config to recreate itself
      kubernetes.core.k8s:
        api_version: imageregistry.operator.openshift.io/v1
        definition:
          api_version: imageregistry.operator.openshift.io/v1
          kind: Config
          metadata:
            name: cluster
          spec:
            managementState: Managed
            rolloutStrategy: Recreate
            replicas: 1
            storage:
              pvc:
                claim: image-registry-storage
      register: image_registry_config

    - name: If image-registry was changed, wait 5 minutes before proceeding so the operator can reconfigure
      pause:
        minutes: 5
      when: image_registry_config.changed


    - name: Start install wait-for install-complete (40 minute time-out each retry)
      shell: "{{ vars.openshift_download_dir }}/openshift-install --dir {{ vars.openshift_config_dir }} wait-for install-complete --log-level=debug"
      register: deploy_install_results
      retries: 2
      delay: 5
      until: '"level=info msg=Install complete!" in deploy_install_results.stderr_lines'
      ignore_errors: yes

    - name: Fail / halt if there is an error with command parameters
      fail:
        msg: "Error with command parameters, help is shown"
      when: '"Wait for install-time events." in deploy_install_results.stdout_lines'

    - name: Show wait-for install-complete output
      debug:
        msg: "{{ ['Command executed in ' + deploy_install_results.attempts | string + ' tries, output:',
                  '-----------------------------------'] +
                  deploy_install_results.stderr_lines }}"


    - name: List Nodes
      shell: oc get nodes
      delegate_to: localhost
      register: cli_nodes_results

    - debug:
        msg: "{{ ['Nodes:'] + cli_nodes_results.stdout_lines }}"

    - name: List ClusterOperators
      shell: oc get co
      delegate_to: localhost
      register: cli_co_results

    - debug:
        msg: "{{ ['ClusterOperators:'] + cli_co_results.stdout_lines }}"

    - name: List ClusterVersion
      shell: oc get clusterversion
      delegate_to: localhost
      register: cli_cv_results

    - debug:
        msg: "{{ ['ClusterVersion:'] + cli_cv_results.stdout_lines }}"



- name: Power on infra nodes
  hosts: openshift_upi_vms_infra_nodes
  gather_facts: false
  vars_files:
  - passwords/ansible_vault.yml
  - vars/network-config.yml
  tasks:

  - name: Start the infra nodes
    tags: deploy_cluster_infra
    block:

    - name: Power on infra nodes
      retries: 3
      delay: 10
      community.vmware.vmware_guest:
        validate_certs: "{{ validate_certs | default(false) }}"
        hostname: "{{ vcenter_hostname | default(lookup('env', 'VMWARE_HOST')) }}"
        username: "{{ vcenter_username | default(lookup('env', 'VMWARE_USER')) }}"
        password: "{{ vcenter_password | default(lookup('env', 'VMWARE_PASSWORD')) }}"
        name: "{{ inventory_hostname_short }}"
        state: poweredon
      delegate_to: localhost
      register: infra_node_boot

    - name: If booted, wait 5 minutes before proceeding so the node can bootstrap
      pause:
        minutes: 5
      when: infra_node_boot.changed



- name: Deploy dedicated OpenShift infra nodes
  hosts: localhost
  gather_facts: false
  vars_files:
  - passwords/ansible_vault.yml
  - vars/network-config.yml
  - group_vars/openshift_upi_vms.yml
  - group_vars/vcenter_vms.yml
  environment:
    KUBECONFIG: "{{ vars.openshift_config_dir }}/auth/kubeconfig"
  tasks:

  - name: Deploy infra nodes
    tags: deploy_cluster_infra
    block:

   
    - name: 'Set # of nodes to wait for'
      set_fact:
        number_of_ready_nodes: "{{ 0 + groups.openshift_upi_vms_master_nodes | length | int + groups.openshift_upi_vms_worker_nodes | length | int + groups.openshift_upi_vms_infra_nodes | length | int}}"

    - name: Accept infra node bootstrap & node serving CSRs
      ansible.builtin.include_tasks: tasks/deploy_openshift_accept_csr_till_all_nodes_ready.yml

    - name: Mark infra nodes unschedulable with a proper taint
      kubernetes.core.k8s:
        api_version: v1
        definition:
          api_version: v1
          kind: Node
          metadata:
            name: "{{ hostvars[item].inventory_hostname_short }}"
            labels:
              node-role.kubernetes.io/infra: ""
          spec:
            taints:
            - effect: NoSchedule
              key: node-role.kubernetes.io/infra
      with_items: "{{ groups.openshift_upi_vms_infra_nodes }}"


    - name: Deploy a new default router definition allowing scheduling on infra nodes only, as json to prevent issues on number of replicas not being an int
      kubernetes.core.k8s:
        state: present
        template: 'templates/openshift_4_router-default.yml'


    - name: Allow taint for 'image-registry' ClusterOperator to run on infra nodes
      kubernetes.core.k8s:
        api_version: imageregistry.operator.openshift.io/v1
        definition:
          api_version: imageregistry.operator.openshift.io/v1
          kind: Config
          metadata:
            name: cluster
          spec:
            tolerations:
            - effect: NoSchedule
              key: node-role.kubernetes.io/infra
              operator: "Exists"
            nodeSelector:
              node-role.kubernetes.io/infra: ""
            affinity:
              podAffinity: 
                preferredDuringSchedulingIgnoredDuringExecution:
                - weight: 100
                  podAffinityTerm:
                    topologyKey: "kubernetes.io/hostname"
                    namespaces:
                    - "openshift-image-registry"


    - name: Deploy a new monitoring ConfigMap definition allowing scheduling on infra nodes only
      kubernetes.core.k8s:
        state: present
        template: 'templates/openshift_4_cluster-monitoring-config.yml'
      register: monitoring_configmap

    - name: If image-registry was changed, wait 5 minutes before proceeding so the operator can reconfigure
      pause:
        minutes: 5
      when: monitoring_configmap.changed


    - name: List Nodes
      shell: oc get nodes
      delegate_to: localhost
      register: cli_nodes_results

    - debug:
        msg: "{{ ['Nodes:'] + cli_nodes_results.stdout_lines }}"

    - name: List ClusterOperators
      shell: oc get co
      delegate_to: localhost
      register: cli_co_results

    - debug:
        msg: "{{ ['ClusterOperators:'] + cli_co_results.stdout_lines }}"



  # ToDo: Creating a separate /var partition
  # ToDo: Updating the bootloader using bootupd
  # ToDo: test MachineNetwork in 10.255.254.0/24
  # ToDo: Logging

